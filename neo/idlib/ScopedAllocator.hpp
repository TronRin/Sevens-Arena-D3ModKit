/*
 * A scoped allocator, that automatically frees all memory it allocated when it
 * goes out of scope, so it can be used as a safer[*] alloca()- (or VLA-) alternative.
 * It usually allocates memory in an existing memory block, stack-like, but if
 * allocations of >3MB are requested, it's allocated on the heap instead - of course
 * the memory is still automatically freed.
 * You can use multiple ScopedAllocator instances per function, but you must only
 * use the currently inner-most one to allocate (see examples below).
 *
 * It supports both allocating proper C++ classes, their constructors and
 * destructors will be called as expected (use AllocArray<MyClass>(42, arg1, arg2)
 *  or AllocOne<MyClass>(arg1, arg2)), and POD (plain old data) types that don't
 * require their constructors/destructors to be called and thus can avoid overhead
 * (e.g. AllocPODs<MySimpleType>(42), AllocPODs<MySimpleType>(42, initVal)
 *  and AllocPODsZeroed<MySimpleType>(42)).
 *
 * Usage: Include this header wherever you want to use dg::ScopedAllocator.
 *  In *one* of the source files, #define DG_SCOPEDALLOCATOR_IMPL before the #include.
 *  The dg::ScopedAllocator class is used like:
 *    dg::ScopedAllocator sa;
 *    MyType* mtArray = sa.AllocArray<MyType>(42, "constructor arg", 1.23f);
 *  This allocates an array MyType with 42 elements, each element's constructor
 *  is called with ("constructor arg", 1.23f).
 *  mtArray is automatically freed once sa goes out of scope, for example at the
 *  end of the function.
 *  NOTE that it is ok to have multiple ScopedAllocators in the same function,
 *  but you only ever must use the innermost one (or, if you've got two in the
 *  same scope, the most recent one).
 *  A "scope" is anything between curly braces ("{...}"), like a function, or the
 *  body of a loop or an if/else statement. A scope starts with "{" and ends with "}",
 *  so local variables defined in a scope are destroyed at the scopes "}" - so
 *  if you have a dg::ScopedAllocator in a scope, it and everything that was allocated
 *  from it will be destroyed/freed at the scopes end ("}").
 *
 *  See below this big comment block for a more elaborate code example.
 *
 *
 * [*] It's "safer" insofar as, unlike alloca() and similar, it can't make your
 *     stack grow further than it's supposed to (which can lead to crashes or worse)
 *     and supports arbitrary allocation sizes (due to falling back to the Heap
 *     for big allocations, using malloc()-like functions then), but still nasty
 *     things can happen if you overflow the returned buffers (write beyond their
 *      end, or before their start address).
 *
 * (C) 2023 Daniel Gibson
 *
 * // TODO: license text
 */

#if 0 // Example

#define DG_SCOPEDALLOCATOR_IMPL // only set this define in *one* of your source files!
#include "ScopedAllocator.hpp"

#include <stdio.h> // example-code uses printf()
#include <string>  // example-code uses std::string
#include <utility> // example-code uses std::move()

// just two types that are used in the sample code below

// vec3 is a simple POD (plain-old-data) -like type for 3D vectors that only
// uses constructors for convenience and doesn't need them or the destructor
// or assignment/move constructors/operators to work as intended
struct vec3 {
	float x, y, z;
	vec3() : x(0.0f), y(0.0f), z(0.0f) {}
	vec3(float v) : x(v), y(v), z(v) {}
	vec3(float x_, float y_, float z_) : x(x_), y(y_), z(z_) {}
	~vec3() {
		// only for illustration, if vec3 is allocated as POD, this will *not* get called
		// usually you'd just leave out the destructor
		printf("vec3(%f, %f, %f) destructor called\n", x, y, z);
	}
	// copy constructor can just be the default generated by the compiler
};

// my class is a non-trivial datatype that needs to be constructed and destructed
// properly to work as intended (not crash or leak memory, ...) and also uses
// copy- and move- constructors/assignment operators
class MyClass {
	std::string val;
public:
	MyClass() : val("default constructed") {}
	MyClass(const char* s) : val(s) {}
	MyClass(const MyClass& other) : val(other.val) {} // copy-constructor
	MyClass(MyClass&& other) : val(std::move(other.val)) { // move-constructor
		other.val = "content was moved by another constructor"; // for demonstration
	}

	~MyClass() {
		printf("MyClass(\"%s\") destructor called\n", val.c_str());
	}

	MyClass& operator=(const MyClass& other) { // copy assignment operator
		val = other.val;
		return *this;
	}

	MyClass& operator=(MyClass&& other) { // move assignment operator
		val = std::move(other.val);
		other.val = "content was moved by another assignment operator"; // for demonstration, could just remain empty
		return *this;
	}
};

void fun() {
	dg::ScopedAllocator sa1;
	// allocate array of 10 MyClass objects, default-constructed
	MyClass* arr = sa1.AllocArray<MyClass>(10);
	// start a new scope
	{
		// allocate array of 3 MyClass objects, each constructor called with "boo" argument.
		// yes, when arr2 goes out of scope you can't reach that memory anymore,
		// but it'll still only be freed when sa1 goes out of scope at the end of the function
		MyClass* arr2 = sa1.AllocArray<MyClass>(3, "booh!");

		dg::ScopedAllocator sa2;

		// NOTE: now sa2 is the innermost ScopedAllocator, so until the end of this scope
		//   you must not call sa.AllocateArray() or similar (if you do, NULL will be
		//    returned and, unless assertions are disabled, an assertion will happen).

		// NOTE: this example assumes that vec3 is a type that doesn't do anything important in
		//   constructors or destructors but either has none or only uses them for convenience

		// allocate array of 4 vec3 objects, each constructor called with (1.0f, 2.0f, 3.0f)
		vec3* vecs = sa2.AllocArray<vec3>(4, 1.0f, 2.0f, 3.0f);

		// allocate array of 4 vec3 objects, but they're completely uninitialized,
		// similar to vec3* morevecs = (vec3*)alloca(4*sizeof(vec3);
		vec3* morevecs = sa2.AllocPODs<vec3>(4);
		// manually initialize the morevecs array, each element getting a different value
		for(int i=0; i<4; ++i) {
			morevecs[i] = vec3(i, 0.0f, 0.0f);
		}

		// allocate array of 3 MyClass objects, each constructor called with "huh?"
		MyClass* arr3 = sa2.AllocArray<MyClass>(3, "huh?");

		// this inner scope ends, so everything allocated with sa2 gets freed:
		// arr3 gets freed after the destructor (~MyClass()) was called on each element
		// morevecs gets freed (but no destructor is called, because it was allocated as PODs!)
		// vecs gets freed after the destructor (~vec3()) was called on each element
		// So the following gets printed at this point:
		// 'MyClass("huh?") destructor called' (3 times; from arr3)
		// 'vec3(1.0, 2.0, 3.0) destructor called' (4 times; from vecs)
		// nothing from morevecs, because it was allocated as POD so no destructor is called
	}
	printf("\ninner scope over\n");

	MyClass obj("I like to move it!");
	// Note: the scope above ended, so sa1 can be used again
	// create one MyClass object and move obj into it (call move constructor)
	// move constructors can only be called with AllocOne(), because it make no
	// sense to move one object into a whole array of objects
	MyClass* oneObj = sa1.AllocOne<MyClass>(std::move(obj));


	// obj gets destructed (because it's a normal stack variable that was created after sa1)
	// oneObj gets destructed and freed (because it's the last object allocated by sa1)
	// arr2 gets destructed (destructor of each element called) and freed
	// arr gets destructed and freed after the destructor has been called on each element
	// So at this point (on exiting fun()), the following gets printed:
	// 'MyClass("content was moved by another constructor") destructor called' (for obj)
	// 'MyClass("I like to move it!") destructor called' (for oneObj, that the original obj was moved into)
	// 'MyClass("booh!") destructor called' (3 times, for arr2)
	// 'MyClass("default constructed") destructor called' (10 times, for arr)

	// => as you can see, the objects are destroyed in the reverse order they were allocated,
	//    THE ONLY THING TO KEEP IN MIND is that all objects allocated
	//    with the same ScopedAllocator are destroyed together, so normal local
	//    variables (like "obj" above) are destroyed either before or after them
	//    (depending on if they were declared after or before the ScopedAllocator),
	//    *not* e.g. between oneObj and arr2.
}


void fun2() {
	// TODO: example with if and a loop
}

#endif // 0
 
#ifndef _DG_SCOPEDALLOCATOR_HPP_
#define _DG_SCOPEDALLOCATOR_HPP_

#include <string.h> // memset
#include <stdint.h>
#include <assert.h>

namespace dg { namespace scal_impl {
// custom dummy-type for custom placement new
struct ScAlNewDummy {};
}}

// custom placement new (with custom dummy-type) so I can avoid #include <new>
inline void* operator new(size_t s, dg::scal_impl::ScAlNewDummy d, void* mem) { return mem; }
inline void operator delete(void*, dg::scal_impl::ScAlNewDummy, void* ) {} // only for symmetry, not actually used

namespace dg {
	
	typedef unsigned char byte;
	
	static constexpr size_t MAX_SIZE_T = ~(size_t(0));
	static_assert(MAX_SIZE_T > 0, "size_t is supposed to be unsigned!");

// this namespace contains implementation details - you shouldn't use
// those functions and types (ScopedAllocator uses them internally)
namespace scal_impl {

/*
 * General Design of the Implementation
 *
 * Each thread has a PerThreadAllocator in TLS (thread-local storage),
 * creating a ScopedAllocator automatically references it.
 * Each PerThreadAllocator has one or more MemBlocks of a fixed size (MEMBLOCK_SIZE, maybe 8MB),
 * if one is full, a second (third, ...) is created as needed, up to MAX_NUM_MEMBLOCKS (8)
 * The memory of a MemBlock contains Chunks of memory, each representing an allocation
 * and aligned to 16 bytes. Each Chunk starts with a ChunkHeader followed by its actual memory
 * (as returned to the user), unless the user requested too much memory (> MAX_CHUNK_DATA_SIZE),
 * then the memory returned to the user is allocated through a normal heap allocator
 * and the ChunkHeader is just followed by a pointer to that memory and its size (struct ExtMem).
 *
 * All memory allocated through a ScopedAllocator is freed by the ScopedAllocator's destructor,
 * so it's kinda like stack memory
 */

enum {
	MEMBLOCK_SIZE = 8*1024*1024, // 8MB
	// allocations for more than MAX_CHUNK_DATA_SIZE bytes use the heap (malloc() or similar)
	MAX_CHUNK_DATA_SIZE = 3*1024*1024, // 3MB
};

byte* HeapAlloc(size_t size, size_t alignment = 16);
void HeapFree(void* mem);

struct alignas(16) ChunkHeader {
	
	enum Flags : uint32_t {
		IS_EXTERNALLY_ALLOCATED = 1 << 24, // in that case, data is a struct ExtMem
		// TODO: as long as MAX_CHUNK_DATA_SIZE < 8MB, its length can be stored in 23 bits,
		//       leaving one for an externally allocated flag and 8 for an offset for alignment
		// => could store an offset here to allow custom >16 byte alignment per allocation
		
		FLAGS_MASK = uint32_t(255) << 24,
		SIZE_MASK = ~FLAGS_MASK
	};
	
	// TODO: make size int32_t and use -1 for "externally allocated", if we don't need any other flags?
	// TODO: OTOH, could also use some bits to store an offset, for >16byte alignments
	uint32_t flags_size;
	uint32_t offsetToPrev; // = 0
	
	// TODO: allow alignments > 16 bytes? then we need to store an offset within data/em->dataPtr, for destructFun() and free()

	// this can be used to properly call the destructor(s) of the element(s) held by this block
	typedef void (DestructFunc)(void* data, size_t dataSize);
	DestructFunc* destructFunc; // = nullptr

	// used for data allocated externally (with malloc() or similar)
	// because the requested size was too big. stored in data[], if flags_size & IS_EXTERNALLY_ALLOCATED
	struct ExtMem {
		void* dataPtr;
		size_t dataSize;
	};

	// flexible array member to hold the data (it's right after this object)
	alignas(16) byte data[];
	

	void* GetData() {
		if((flags_size & IS_EXTERNALLY_ALLOCATED) == 0)
			return data;
		ExtMem* em = reinterpret_cast<ExtMem*>(data);
		return em->dataPtr;
	}
	
	// size of this block within the stack memory
	// if IS_EXTERNALLY_ALLOCATED, this will be sizeof(AllocatedMemBlock) + sizeof(void*),
	// rounded up to the next multiple of 16
	uint32_t GetBlockSize() {
		uint32_t dataSize = ((flags_size & IS_EXTERNALLY_ALLOCATED) == 0)
			? (flags_size & SIZE_MASK) : sizeof(ExtMem);
		
		dataSize = (dataSize + 15) & ~uint32_t(15); // round to multiple of 16
		
		return sizeof(ChunkHeader) + dataSize;
	}
	
	size_t GetDataSize() {
		if((flags_size & IS_EXTERNALLY_ALLOCATED) == 0)
			return (flags_size & SIZE_MASK);

		ExtMem* em = reinterpret_cast<ExtMem*>(data);
		return em->dataSize;
	}

	void Free();
};

static_assert((sizeof(ChunkHeader) & 15) == 0, "ChunkHeader's size should be (a multiple of) 16 bytes!");

struct MemBlock {
	byte* memory = nullptr;

	uint32_t lastOffset = 0; // offset of last allocated chunk
	uint32_t curOffset = 0;  // offset of next chunk to allocate

	void Init() {
		assert(memory == nullptr);
		memory = HeapAlloc(MEMBLOCK_SIZE);
		lastOffset = curOffset = 0;
	}

	void Destroy() {
		FreeUntilOffset(0);
		HeapFree(memory);
		memory = nullptr;
	}

	// frees all objects in this memblock until (including) offset, starting at the end
	void FreeUntilOffset(uint32_t offset);

	byte* Allocate(size_t size, ChunkHeader::DestructFunc* destructFunc);
};


struct PerThreadAllocator
{
	int curMemBlockIdx = 0;
	enum {
		MAX_NUM_MEMBLOCKS = 8,
		MAX_MEMBLOCK_IDX = MAX_NUM_MEMBLOCKS - 1
	};
	MemBlock memBlocks[MAX_NUM_MEMBLOCKS]; // TODO: dynamic?
	// how many ScopedAllocators are currently exist in this thread,
	// used to help ensure you only use the currently innermost ScopedAllocator
	int scopeDepth = 0;

	PerThreadAllocator() {
		memBlocks[0].Init();
	}

	~PerThreadAllocator() {
		for(MemBlock& mb : memBlocks) {
			if(mb.memory != nullptr) {
				mb.Destroy();
			}
		}
	}

	byte* GrowAndAllocate(size_t size, scal_impl::ChunkHeader::DestructFunc* destructFunc);

	byte* Allocate(size_t size, scal_impl::ChunkHeader::DestructFunc* destructFunc)
	{
		if(size == 0)
			return nullptr;
		byte* ret = memBlocks[curMemBlockIdx].Allocate(size, destructFunc);
		if(ret == nullptr) { // current memblock is full, allocate new one if possible
			ret = GrowAndAllocate(size, destructFunc);
		}
		assert(ret != nullptr && "if ret is NULL, you're either out of system memory or (more likely) out of memBlocks in PerThreadAllocator");
		return ret;
	}

	void FreeUntil(int startMemBlockIdx, uint32_t offsetInStartMemBlock) {
		assert(curMemBlockIdx >= startMemBlockIdx);
		while(curMemBlockIdx > startMemBlockIdx) {
			// TODO: only keep one unused memblock around? then do this:
			//if(curMemBlockIdx < MAX_MEMBLOCK_IDX && memBlocks[curMemBlockIdx+1].memory != nullptr) {
			//	memBlocks[curMemBlockIdx+1].Destroy();
			//}
			memBlocks[curMemBlockIdx].FreeUntilOffset(0);
			--curMemBlockIdx;
		}
		assert(curMemBlockIdx == startMemBlockIdx);
		memBlocks[curMemBlockIdx].FreeUntilOffset(offsetInStartMemBlock);
	}
};

} //namespace scal_impl

// the public interface of the ScopedAllocator, meaning, the part you should use
// everything allocated with an ScopedAllocator instance is freed when it goes out of scope
class ScopedAllocator
{
	static thread_local scal_impl::PerThreadAllocator tlsPerThreadAlloc;
	
	// reference to tlsPerThreadAlloc so TLS only needs to be looked up
	// once per ScopedAllocator instance, in the constructor
	scal_impl::PerThreadAllocator& perThreadAlloc;
	int myScopeDepth; // PerThreadAllocator::scopeDepth, used to ensure only the innermost ScopedAllocator is used
	
	// the following two are used to free all memory allocated up to the point
	// PerThreadAllocator was at when this ScopedAllocator was created
	int startMemBlockIndex; // PerThreadAllocator::curMemBlockIdx at construction time
	uint32_t startMemBlockOffset; // MemBlock::curOffset in that memory block at construction time

	// helper function that calls the destructor of each array element
	// of an array of type T that fits into dataLen bytes
	template<typename T>
	static void destructArr(void* data, size_t dataLen)
	{
		T* arr = static_cast<T*>(data);
		size_t numElems = dataLen/sizeof(T);
		for(size_t i = 0; i<numElems; ++i) {
			arr[i].~T();
		}
	}

	// isn't copyable or movable
	ScopedAllocator(const ScopedAllocator&) = delete;
	ScopedAllocator(ScopedAllocator&&) = delete;
	ScopedAllocator& operator=(const ScopedAllocator&) = delete;
	ScopedAllocator& operator=(ScopedAllocator&&) = delete;

public:
	ScopedAllocator() : perThreadAlloc( tlsPerThreadAlloc )
	{
		++perThreadAlloc.scopeDepth;
		myScopeDepth = perThreadAlloc.scopeDepth;
		startMemBlockIndex = perThreadAlloc.curMemBlockIdx;
		startMemBlockOffset = perThreadAlloc.memBlocks[startMemBlockIndex].curOffset;
	}

	~ScopedAllocator() {
		perThreadAlloc.FreeUntil(startMemBlockIndex, startMemBlockOffset);
		--perThreadAlloc.scopeDepth;
	}


	// allocate a raw buffer of given size (in bytes)
	void* AllocRaw(size_t size) {
		if(myScopeDepth != perThreadAlloc.scopeDepth) {
			assert(0 && "Only allocate from the innermost ScopedAllocator!");
			return nullptr;
		}
		perThreadAlloc.Allocate(size, nullptr);
		return nullptr;
	}
	
	// Allocate a cleared raw buffer of given size (in bytes),
	// it will be cleared with zeroes
	void* AllocRawZeroed(size_t size) {
		void* ret = AllocRaw(size);
		if(ret != nullptr && size > 0) {
			memset(ret, 0, size);
		}
		return ret;
	}

	// Allocate an array of an arbitrary C++ type.
	// (use like MyType* arr = myScopedAllocator.AllocArray<MyType>(42, "constructor arg");)
	// The "args" are passed to the constructor of each array element
	// (they're not forwarded though, so you can't call move-constructors and such..
	//  but that wouldn't make sense with more than one element anyway; for one element you can use AllocOne())
	template<typename T, typename... Args>
	T* AllocArray(size_t numElems, Args&&... args)
	{
		if(myScopeDepth != perThreadAlloc.scopeDepth) {
			assert(0 && "Only allocate from the innermost ScopedAllocator!");
			return nullptr;
		}
		// make sure numElems*sizeof(T) won't overflow
		if( numElems < MAX_SIZE_T / sizeof(T)) {
			size_t bufSize = numElems*sizeof(T);
			// if T has a "trivial" destructor, no DestructFunc is needed
			// NOTE: if your compiler doesn't support __has_trivial_destructor(),
			// use std::is_trivially_destructible<T>::value from <type_traits> instead
			// (at least GCC, Clang and Visual C++ support __has_trivial_destructor)
			scal_impl::ChunkHeader::DestructFunc* destr = __has_trivial_destructor(T) ? nullptr : destructArr<T>;
			byte* data = perThreadAlloc.Allocate(bufSize, destr);
			if(data != nullptr) {
				for(size_t i=0; i<numElems; ++i) {
					new(scal_impl::ScAlNewDummy(), data + i*sizeof(T)) T(args...);
				}
			}
			return reinterpret_cast<T*>(data);
		} else { // numElems >= MAX_SIZE_T / sizeof(T)
			assert(0 && "numElems too big, buffer size would exceed address space!");
			return nullptr;
		}
	}

	// Allocate a single element of given type, "args" are forwarded as constructor arguments
	// (=> can even be used to call a move constructor, or other constructor taking forwarding references)
	template<typename T, typename... Args>
	T* AllocOne(Args&&... args)
	{
		if(myScopeDepth != perThreadAlloc.scopeDepth) {
			assert(0 && "Only allocate from the innermost ScopedAllocator!");
			return nullptr;
		}
		size_t bufSize = sizeof(T);
		// see AllocArray() for explanation of the following line
		scal_impl::ChunkHeader::DestructFunc* destr = __has_trivial_destructor(T) ? nullptr : destructArr<T>;
		byte* data = perThreadAlloc.Allocate(bufSize, destr);
		if(data != nullptr) {
			// note that unlike AllocArray() this supports calling move constructors
			// those only make sense for single elements
			new(scal_impl::ScAlNewDummy(), data) T( static_cast<Args&&>(args)... ); // like std::forward (I hope :-p)
		}
		return reinterpret_cast<T*>(data);
	}

	// Allocate an array of POD (plain old data) elements (or just a single one if you like).
	// !! only use for types that work without calling a constructor or destructor !!
	// ! the returned elements are completely uninitialized, i.e. will contain random data !
	// => Use the version of AllocPODs() with an initVal or AllocPODsZeroed()
	//    if you want to receive deterministic data!
	template<typename T>
	T* AllocPODs(size_t numElems=1)
	{
		if(myScopeDepth != perThreadAlloc.scopeDepth) {
			assert(0 && "Only allocate from the innermost ScopedAllocator!");
			return nullptr;
		}
		// make sure numElems*sizeof(T) won't overflow
		if( numElems < MAX_SIZE_T / sizeof(T)) {
			size_t bufSize = numElems*sizeof(T);
			byte* data = perThreadAlloc.Allocate(bufSize, nullptr);
			return reinterpret_cast<T*>(data);
		} else { // numElems >= MAX_SIZE_T / sizeof(T)
			assert(0 && "numElems too big, buffer size would exceed address space!");
			return nullptr;
		}
	}

	// Allocate an array of (as) POD (plain old data) elements (or just a single one if you like).
	// !! only use for types that work without calling a constructor or destructor !!
	// each element will be initialized with "initVal" (ret[i] = initVal;)
	template<typename T>
	T* AllocPODs(size_t numElems, const T& initVal)
	{
		T* ret = AllocPODs<T>(numElems);
		if(ret != nullptr) {
			for(size_t i=0; i<numElems; ++i) {
				ret[i] = initVal;
			}
		}
		return ret;
	}

	// Allocating an array of (as) POD (plain old data) elements (or just a single one if you like).
	// !! only use for types that work without calling a constructor or destructor !!
	// the whole returned array will have been cleared with memset(ret, 0, numElems*sizeof(T));
	template<typename T>
	T* AllocPODsZeroed(size_t numElems=1)
	{
		T* ret = AllocPODs<T>(numElems);
		if(ret != nullptr && numElems > 0) {
			memset(ret, 0, numElems*sizeof(T));
		}
		return ret;
	}

};


} //namespace dg
#endif // _DG_SCOPEDALLOCATOR_HPP_

// ########## non-inline Implementation ##########

#ifdef DG_SCOPEDALLOCATOR_IMPL

#include <stdlib.h> // malloc()/posix_memalign()/aligned_alloc(), free()
#ifdef _WIN32
#include <malloc.h> // _aligned_malloc(), _aligned_free()
#endif

namespace dg {
//static
thread_local scal_impl::PerThreadAllocator ScopedAllocator::tlsPerThreadAlloc;

namespace scal_impl {

byte* HeapAlloc(size_t size, size_t alignment)
{
#ifdef _WIN32
	// Windows: https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/aligned-malloc?view=msvc-140
	return (byte*)_aligned_malloc(size, alignment);
#elif defined(__unix__)
	void* ret = nullptr;
	if(posix_memalign(&ret, alignment, size) == 0) {
		return (byte*)ret;
	}
	return nullptr;

	// NOTE: Mac OSX only supports posix_memalign() since 10.6, I think.
	//       So this might need another #elif defined(__APPLE__) special case,
	//       if anyone wants to support ancient OSX..
	//       OTOH, OSX malloc() guarantees 16byte alignment at least, and no idea
	//       if C++11 is even supported on <= 10.5

#else // C11, C++17 (except on Windows, which doesn't seem to support it)

	return (byte*)::aligned_alloc(alignment, size);

	// if this doesn't work on your platform, add another #elif with an appropriate implementation
#endif
}

void HeapFree(void* mem)
{
#ifdef _WIN32
	// https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/aligned-free?view=msvc-140
	_aligned_free(mem);
#else // unix, C11, C++17 on sane platforms
	::free(mem);
#endif
}

void ChunkHeader::Free() {
	void* mem = GetData();
	if(destructFunc != nullptr) {
		destructFunc(mem, GetDataSize());
		destructFunc = nullptr;
	}
	if(flags_size & IS_EXTERNALLY_ALLOCATED) {
		HeapFree(mem);
	}
	flags_size = 0;
}

void MemBlock::FreeUntilOffset(uint32_t offset) {
	uint32_t curOS = curOffset;
	if(curOS == offset)
		return;
	uint32_t lastOS = lastOffset;
	byte* mem = memory;
	ChunkHeader* chunk;
	while(curOS > offset) {
		assert(curOS > lastOS);
		byte* chunkMem = mem + lastOS;
		assert(chunkMem >= mem);
		chunk = (ChunkHeader*)chunkMem;
		chunk->Free();

		curOS = lastOS;
		assert(chunk->offsetToPrev <= lastOS);
		lastOS -= chunk->offsetToPrev;
	}

	curOffset = curOS;
	lastOffset = lastOS;
	assert(offset != 0 || (lastOffset == 0 && curOffset == 0));
}

byte* MemBlock::Allocate(size_t size, ChunkHeader::DestructFunc* destructFunc) {
	uint32_t chunkSize = (size <= MAX_CHUNK_DATA_SIZE) ? size : sizeof(ChunkHeader::ExtMem);
	chunkSize = (chunkSize + 15) & ~uint32_t(15); // make sure chunkSize is multiple of 16
	chunkSize += sizeof(ChunkHeader);
	if(lastOffset + chunkSize > MEMBLOCK_SIZE)
		return nullptr;

	uint32_t offsetToPrev = curOffset - lastOffset;

	lastOffset = curOffset;
	curOffset += chunkSize;

	byte* ret = nullptr;

	ChunkHeader* chunk = (ChunkHeader*)(memory+lastOffset);
	chunk->offsetToPrev = offsetToPrev;
	chunk->destructFunc = destructFunc;
	if(size <= MAX_CHUNK_DATA_SIZE) {
		chunk->flags_size = size;
		ret = chunk->data;
	} else {
		chunk->flags_size = ChunkHeader::IS_EXTERNALLY_ALLOCATED;
		ChunkHeader::ExtMem* em = (ChunkHeader::ExtMem*)chunk->data;
		ret = HeapAlloc(size);
		em->dataPtr = ret;
		em->dataSize = size;
	}

	return ret;
}

byte* PerThreadAllocator::GrowAndAllocate(size_t size, scal_impl::ChunkHeader::DestructFunc* destructFunc)
{
	if(curMemBlockIdx < MAX_MEMBLOCK_IDX) { // we still have free memblocks
		++curMemBlockIdx;
		if(memBlocks[curMemBlockIdx].memory == nullptr) {
			memBlocks[curMemBlockIdx].Init();
		}
		return memBlocks[curMemBlockIdx].Allocate(size, destructFunc);
	}
	return nullptr;
}

}} //namespace dg::scal_impl
#endif // DG_SCOPEDALLOCATOR_IMPL
